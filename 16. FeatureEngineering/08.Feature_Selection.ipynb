{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a crucial step in building a machine learning model, helping to enhance its performance by reducing noise from irrelevant or redundant data. There are three main types of feature selection methods: filter, wrapper, and embedded\n",
    "\n",
    "- [**Filter methods**](https://medium.com/@Hassane_01/master-feature-selection-part1-filter-methods-ca5ede964ebf) prioritize simplicity, using criteria like variance or correlation to select features without needing to build models. They're computationally fast and model-agnostic but might overlook complex multivariate relationships.\n",
    "\n",
    "    Advantages\n",
    "    - They are computationally inexpensive, since they do not involve testing the subsetted features using a model.\n",
    "    - They can work for any type of machine learning model.\n",
    "\n",
    "    Disadvantages\n",
    "\n",
    "    - It is more difficult to take multivariate relationships into account because we are not evaluating model performance. For example, a variable might not have much predictive power on its own, but can be informative when combined with other variables.\n",
    "    - They are not tailored toward specific types of models.\n",
    "\n",
    "- [**Wrapper methods**](https://medium.com/p/20be72b3f5c6), on the other hand, assess subsets of features based on the model's performance, identifying the optimal combination for the specific problem at hand. They're effective at capturing multivariate interactions but are computationally intensive.\n",
    "\n",
    "    Advantages\n",
    "\n",
    "    - They can determine the optimal set of features that produce the best results for a specific machine learning problem.\n",
    "    - They can better account for multivariate relationships because model performance is evaluated.\n",
    "    \n",
    "    Disadvantages\n",
    "    \n",
    "    - They are computationally expensive because the model needs to be re-fitted for each feature set being tested.\n",
    "\n",
    "- [**Embedded methods**](https://medium.com/p/a1df09b677fa) combine feature selection with model training, thus theyâ€™re more efficient than wrapper methods while still considering multivariate relationships. Techniques like regularization and tree-based feature importance fall into this category.\n",
    "\n",
    "    Advantages\n",
    "    \n",
    "    - Like wrapper methods, they can optimize the feature set for a particular model and account for multivariate relationships.\n",
    "    - They are also generally less computationally expensive because feature selection happens during model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvRoadmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color: #f8f0fa;\n",
    "            border-left: 5px solid #1b4332;\n",
    "            font-family: 'Trebuchet MS', sans-serif;\n",
    "            border-right: 5px solid #1b4332;\n",
    "            padding: 12px;\n",
    "            border-radius: 50px 50px;\n",
    "            color: #1b4332;\n",
    "            text-align:center;\n",
    "            font-size:45px;\"><strong>ðŸ˜ŠTokenizationðŸŒŸ</strong></h1>\n",
    "<hr style=\"border-top: 5px solid #264653;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Tokenization is a fundamental step in Natural Language Processing (NLP) where a text string is split into smaller units called tokens. These tokens can be words, phrases, or even characters, depending on the application. \n",
    "\n",
    "---\n",
    "\n",
    "## Why is Tokenization Important?\n",
    "1. Converts raw text into a format that can be processed by models.\n",
    "2. Preserves the semantic meaning of text.\n",
    "3. Acts as the first step in most NLP pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Tokenization\n",
    "1. **Word Tokenization**\n",
    "   - Splits text into individual words.\n",
    "   - Example: \"Natural Language Processing\" â†’ [\"Natural\", \"Language\", \"Processing\"]\n",
    "\n",
    "2. **Sentence Tokenization**\n",
    "   - Splits text into sentences.\n",
    "   - Example: \"Hello World. Welcome to NLP.\" â†’ [\"Hello World.\", \"Welcome to NLP.\"]\n",
    "\n",
    "3. **Subword Tokenization**\n",
    "   - Breaks words into smaller subunits.\n",
    "   - Example: \"unbelievable\" â†’ [\"un\", \"believ\", \"able\"]\n",
    "\n",
    "4. **Character Tokenization**\n",
    "   - Splits text into individual characters.\n",
    "   - Example: \"NLP\" â†’ [\"N\", \"L\", \"P\"]\n",
    "\n",
    "---\n",
    "\n",
    "## Popular Libraries for Tokenization\n",
    "1. **NLTK (Natural Language Toolkit)**\n",
    "   - `word_tokenize`: For word-level tokenization.\n",
    "   - `sent_tokenize`: For sentence-level tokenization.\n",
    "\n",
    "2. **spaCy**\n",
    "   - Offers efficient and customizable tokenization.\n",
    "   - Provides linguistic features like Part-of-Speech tagging with tokens.\n",
    "\n",
    "3. **Hugging Face Tokenizers**\n",
    "   - Supports modern tokenization techniques for transformers like BERT, GPT, etc.\n",
    "   - Provides subword tokenization and encoding.\n",
    "\n",
    "4. **Regex-based Tokenization**\n",
    "   - Custom tokenization using Python's `re` library.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hassa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Path: ['C:\\\\Users\\\\hassa/nltk_data', 'c:\\\\python\\\\python3126\\\\nltk_data', 'c:\\\\python\\\\python3126\\\\share\\\\nltk_data', 'c:\\\\python\\\\python3126\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\hassa\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK Data Path:\", nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'my', 'name', 'is', 'hassane', '.', 'I', \"'m\", '20', 'years', 'old', '.', 'I', \"'m\", 'from', 'Morroco', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello everyone. my name is hassane. I'm 20 years old. I'm from Morroco.\"\n",
    "\n",
    "sentences = word_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', 'my name is hassane.', \"I'm 20 years old.\", \"I'm from Morroco.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello everyone. my name is hassane. I'm 20 years old. I'm from Morroco.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the module\n",
    "```python \n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'my', 'name', 'is', 'hassane', 'skikri', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello my name is hassane skikri.\")\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Custom Tokenization with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Tokenization in NLP!\"\n",
    "tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenges in Tokenization\n",
    "1. **Ambiguity in Languages**\n",
    "   - Some languages (e.g., Chinese, Japanese) do not use spaces to separate words.\n",
    "2. **Handling Special Characters**\n",
    "   - Symbols, hashtags, and URLs need special treatment.\n",
    "3. **Language-Specific Rules**\n",
    "   - Grammar and semantics vary across languages, affecting tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Tokenization Techniques\n",
    "1. **Byte Pair Encoding (BPE)**\n",
    "   - Used in models like GPT, BERT.\n",
    "   - Combines frequently occurring sequences of characters into subwords.\n",
    "\n",
    "2. **Unigram Language Models**\n",
    "   - Probabilistic approach for subword segmentation.\n",
    "\n",
    "3. **SentencePiece**\n",
    "   - Library for unsupervised text tokenization and subword segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "## Tips for Effective Tokenization\n",
    "1. Choose the right tokenization technique for your use case.\n",
    "2. Preprocess text (lowercasing, removing stop words) before tokenization if needed.\n",
    "3. Use libraries like Hugging Face for modern tokenization needs.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-world Applications\n",
    "1. **Search Engines**\n",
    "   - Tokenization helps in indexing and retrieving documents.\n",
    "2. **Chatbots**\n",
    "   - Tokenization enables better understanding of user queries.\n",
    "3. **Machine Translation**\n",
    "   - Subword tokenization improves translation quality.\n",
    "4. **Text Summarization**\n",
    "   - Sentence tokenization aids in extracting meaningful summaries.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "Tokenization is a critical step in NLP that transforms raw text into meaningful units. By mastering different tokenization techniques and tools, you can significantly enhance your NLP projects.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

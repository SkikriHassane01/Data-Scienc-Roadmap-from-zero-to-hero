{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7caa71c",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color: #f8f0fa;\n",
    "            border-left: 5px solid #1b4332;\n",
    "            font-family: 'Trebuchet MS', sans-serif;\n",
    "            border-right: 5px solid #1b4332;\n",
    "            padding: 12px;\n",
    "            border-radius: 50px 50px;\n",
    "            color: #1b4332;\n",
    "            text-align:center;\n",
    "            font-size:45px;\"><strong>ðŸ˜ŠXGBoost Algorithm AlgorithmðŸŒŸ</strong></h1>\n",
    "<hr style=\"border-top: 5px solid #264653;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782feec8",
   "metadata": {},
   "source": [
    "This notebook demonstrates the implementation of **XGBoost (Extreme Gradient Boosting)**, \n",
    "a powerful gradient boosting method commonly used for structured/tabular data. \n",
    "It provides efficient, accurate predictions by combining decision trees with boosting techniques.\n",
    "\n",
    "In this example, weâ€™ll build a simple classification model using XGBoost on a small dataset, \n",
    "demonstrating data preparation, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63c81f",
   "metadata": {},
   "source": [
    "\n",
    "## Key Concepts in XGBoost\n",
    "\n",
    "1. **Tree Building**: XGBoost builds sequential decision trees to minimize residuals from previous iterations.\n",
    "\n",
    "2. **Regularization**: It includes L1 and L2 regularization to control overfitting.\n",
    "\n",
    "3. **Learning Rate**: Controls the contribution of each new tree, allowing smooth convergence.\n",
    "\n",
    "4. **Hyperparameters**: Adjusting parameters like max depth and subsample fraction improves model flexibility.\n",
    "\n",
    "5. **Early Stopping**: Stops training when performance on validation data no longer improves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31c7e9",
   "metadata": {},
   "source": [
    "\n",
    "## Example Dataset\n",
    "\n",
    "This example uses a small classification dataset with **10 samples** and **3 features**.\n",
    "\n",
    "| Sample | Feature 1 | Feature 2 | Feature 3 | Target |\n",
    "|--------|-----------|-----------|-----------|--------|\n",
    "| 1      | 1.0       | 2.0       | 3.0       | 1      |\n",
    "| 2      | 2.0       | 1.0       | 2.0       | 0      |\n",
    "| 3      | 3.0       | 2.0       | 1.0       | 1      |\n",
    "| 4      | 1.0       | 3.0       | 2.0       | 0      |\n",
    "| 5      | 2.0       | 1.0       | 1.0       | 1      |\n",
    "| 6      | 3.0       | 3.0       | 2.0       | 0      |\n",
    "| 7      | 1.0       | 1.0       | 1.0       | 1      |\n",
    "| 8      | 2.0       | 2.0       | 3.0       | 0      |\n",
    "| 9      | 3.0       | 1.0       | 2.0       | 1      |\n",
    "| 10     | 1.0       | 3.0       | 3.0       | 0      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the dataset\n",
    "data = {\n",
    "    'Sample': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Feature_1': [1.0, 2.0, 3.0, 1.0, 2.0, 3.0, 1.0, 2.0, 3.0, 1.0],\n",
    "    'Feature_2': [2.0, 1.0, 2.0, 3.0, 1.0, 3.0, 1.0, 2.0, 1.0, 3.0],\n",
    "    'Feature_3': [3.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 3.0, 2.0, 3.0],\n",
    "    'Target': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Sample', inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58ce7d",
   "metadata": {},
   "source": [
    "\n",
    "## Train-Test Split\n",
    "\n",
    "Split the data into training and testing sets to evaluate model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd97f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df[['Feature_1', 'Feature_2', 'Feature_3']]\n",
    "y = df['Target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0625bc9",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Train XGBoost Model\n",
    "\n",
    "Train an XGBoost model using the training data, setting key parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f344b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', max_depth=3, learning_rate=0.1, n_estimators=10)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa2851",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Predict and Evaluate\n",
    "\n",
    "Predict on the test set and evaluate using accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
